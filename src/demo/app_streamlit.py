import threading
import streamlit as st
import time
import torch
import cv2
import numpy as np
import tempfile
import os
import pandas as pd
from ultralytics import YOLO
from torchvision import models, transforms
import torch.nn as nn
from PIL import Image
import json
import torch.nn.functional as F
import unicodedata
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
import av
from collections import deque

MODEL_LOCK = threading.Lock()

DET_MIN_CONF = 0.18          # gi·ªØ detection khi webcam m·ªù
UNCERTAIN_MIN_CONF = 0.10    # d∆∞·ªõi m·ª©c n√†y b·ªè h·∫≥n bbox r√°c
SPECIAL_MIN_CONF = {
    "NO_STRAIGHT_RIGHT": 0.65,  # √©p class n√†y ph·∫£i ch·∫Øc m·ªõi hi·ªán (ch·ªëng spam)
}

CLS_MIN_PROB = 0.75          # classifier ch·ªâ override khi r·∫•t ch·∫Øc

def strip_accents(text: str) -> str:
    """B·ªè d·∫•u ti·∫øng Vi·ªát ƒë·ªÉ in b·∫±ng cv2.putText."""
    text_nfkd = unicodedata.normalize("NFD", text)
    return "".join(ch for ch in text_nfkd if unicodedata.category(ch) != "Mn")

# ==================== PAGE CONFIG ====================
st.set_page_config(
    page_title="Traffic Sign Demo System",
    page_icon="üöó",
    layout="wide"
)

# ==================== CONFIG ====================
YOLO_MODEL_PATH = "models/detection/yolov8n_vn_best.pt"

CLASSIFY_MODEL_PATH = "models/classification/efficientnet_best.pth"
CLASS_NAMES_FILE    = "models/classification/class_names_gtsrb.json"

NUM_CLASSES = 43
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ==================== LOAD MODELS ====================
@st.cache_resource
def load_yolo():
    return YOLO(YOLO_MODEL_PATH)

@st.cache_resource
def load_classifier():
    # EfficientNet-B0 backbone
    clf = models.efficientnet_b0(weights=None)
    in_feats = clf.classifier[1].in_features
    clf.classifier[1] = nn.Linear(in_feats, NUM_CLASSES)
    clf.load_state_dict(torch.load(CLASSIFY_MODEL_PATH, map_location=DEVICE))
    clf.eval().to(DEVICE)
    return clf

yolo_model = load_yolo()
clf_model  = load_classifier()

# ==================== RULE: CH·ªà RE-FINE M·ªòT S·ªê BI·ªÇN ====================
# Nh·ªØng t·ª´ kho√° n·∫øu xu·∫•t hi·ªán trong YOLO_label th√¨ m·ªõi cho classifier "chen v√†o"
REFINE_WITH_CLS = [
    "speed",            # v√≠ d·ª• YOLO_label = "Speed limit", "speed-limit-50"...
    "gi·ªõi h·∫°n t·ªëc ƒë·ªô",  # n·∫øu sau n√†y b·∫°n d√πng nh√£n ti·∫øng Vi·ªát
]

# ==================== CLASS NAMES (YOLO - VN) ====================
CLASSES_FILE = "models/detection/classes_vn.json"

if os.path.exists(CLASSES_FILE):
    with open(CLASSES_FILE, "r", encoding="utf-8") as f:
        CLASS_NAMES_SHORT = json.load(f)
else:
    CLASS_NAMES_SHORT = {}


# ==================== CLASS NAMES (GTSRB) ====================
if os.path.exists(CLASS_NAMES_FILE):
    with open(CLASS_NAMES_FILE, "r", encoding="utf-8") as f:
        CLASS_NAMES = json.load(f)
else:
    # fallback: ƒë·∫∑t t√™n class_i n·∫øu ch∆∞a c√≥ file
    CLASS_NAMES = [f"class_{i}" for i in range(NUM_CLASSES)]

# ==================== TRANSFORMS ====================
clf_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])

# ==================== HELPER: CLASSIFY CROP T·ª™ BBOX ====================
def classify_crop_bgr(img_bgr, x1, y1, x2, y2, topk=1):
    """
    C·∫Øt v√πng [x1, y1, x2, y2] t·ª´ img_bgr (BGR),
    ch·∫°y qua EfficientNet-B0 (clf_model),
    tr·∫£ v·ªÅ list [(label, prob), ...] theo Top-k.
    """
    # K√≠ch th∆∞·ªõc ·∫£nh
    h, w = img_bgr.shape[:2]

    # Gi·ªõi h·∫°n to·∫° ƒë·ªô trong khung ·∫£nh
    x1 = max(0, min(int(x1), w - 1))
    x2 = max(0, min(int(x2), w - 1))
    y1 = max(0, min(int(y1), h - 1))
    y2 = max(0, min(int(y2), h - 1))

    if x2 <= x1 or y2 <= y1:
        return None  # bbox l·ªói

    # C·∫Øt ·∫£nh
    crop_bgr = img_bgr[y1:y2, x1:x2]
    if crop_bgr.size == 0:
        return None

    # BGR -> RGB -> PIL
    crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
    pil_img  = Image.fromarray(crop_rgb)

    # Transform gi·ªëng nh∆∞ classification mode
    x = clf_transform(pil_img).unsqueeze(0).to(DEVICE)

    # Ch·∫°y qua EfficientNet
    with torch.no_grad():
        logits = clf_model(x)
        probs  = F.softmax(logits, dim=1)[0]

    # L·∫•y Top-k
    top_probs, top_indices = torch.topk(probs, k=topk)
    top_probs   = top_probs.cpu().numpy()
    top_indices = top_indices.cpu().numpy()

    preds = []
    for idx_i, prob_i in zip(top_indices, top_probs):
        idx_i = int(idx_i)
        name_i = CLASS_NAMES[idx_i] if idx_i < len(CLASS_NAMES) else f"class_{idx_i}"
        preds.append((name_i, float(prob_i)))

    return preds

# ==================== GLOBAL HEADER ====================
st.markdown(
    "<h1 style='text-align: center; margin-bottom: 0.2rem;'>üöó Traffic Sign Demo System</h1>",
    unsafe_allow_html=True,
)
st.markdown(
    "<p style='text-align: center; color: gray;'>"
    "H·ªá th·ªëng minh h·ªça nh·∫≠n di·ªán & ph√¢n lo·∫°i bi·ªÉn b√°o giao th√¥ng s·ª≠ d·ª•ng YOLOv8 v√† EfficientNet-B0."
    "</p>",
    unsafe_allow_html=True,
)
st.markdown("---")

# =========== SIDEBAR: MODE & INFO ===========
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh demo")
    mode = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô:",
        [
            "1Ô∏è‚É£ Vietnam Traffic Sign Detection (YOLOv8)",
            "2Ô∏è‚É£ GTSRB Traffic Sign Classification (EfficientNet-B0)",
            "3Ô∏è‚É£ Unified Pipeline (YOLO + EfficientNet)"
        ]
    )
    st.markdown("---")
    st.markdown("### ‚ÑπÔ∏è H∆∞·ªõng d·∫´n nhanh")
    if mode.startswith("1Ô∏è‚É£"):
        st.write(
            "- Ch·ªçn **Image** ƒë·ªÉ upload m·ªôt ·∫£nh giao th√¥ng.\n"
            "- Ch·ªçn **Video** ƒë·ªÉ upload m·ªôt video ng·∫Øn.\n"
            "- K·∫øt qu·∫£ s·∫Ω hi·ªÉn th·ªã khung bao quanh bi·ªÉn b√°o v√† b·∫£ng t·ªïng h·ª£p."
        )
    elif mode.startswith("2Ô∏è‚É£"):
        st.write(
            "- Upload **·∫£nh ƒë√£ crop s·∫µn** ch·ªâ ch·ª©a bi·ªÉn b√°o.\n"
            "- H·ªá th·ªëng tr·∫£ v·ªÅ l·ªõp d·ª± ƒëo√°n (Top-5) v√† b·∫£ng Top-k."
        )
    else:  # 3Ô∏è‚É£ Unified Pipeline
        st.write(
            "- Ch·ªçn **Image / Video / Webcam** l√†m ngu·ªìn ƒë·∫ßu v√†o.\n"
            "- H·ªá th·ªëng t·ª± ƒë·ªông **ph√°t hi·ªán bi·ªÉn b√°o (YOLOv8)** v√† **ph√¢n lo·∫°i chi ti·∫øt (EfficientNet-B0)**.\n"
            "- M·ªói bi·ªÉn b√°o ƒë∆∞·ª£c hi·ªÉn th·ªã **bounding box, nh√£n cu·ªëi c√πng v√† confidence**.\n"
            "- B·∫£ng k·∫øt qu·∫£ hi·ªÉn th·ªã **nh√£n YOLO, nh√£n classifier, Top-5 prediction**.\n"
            "- C√°c tr∆∞·ªùng h·ª£p **mismatch** gi·ªØa detection v√† classification ƒë∆∞·ª£c ƒë√°nh d·∫•u ƒë·ªÉ ph√¢n t√≠ch.\n"
            "- FPS realtime ƒë∆∞·ª£c hi·ªÉn th·ªã ƒë·ªÉ ƒë√°nh gi√° hi·ªáu nƒÉng h·ªá th·ªëng."
        )


class Mode3WebcamProcessor(VideoProcessorBase):
    """
    Realtime webcam processor for Mode 3:
    YOLO -> crop -> classifier -> (optional refine) -> draw -> FPS
    """

    def __init__(self):
        # runtime options (set t·ª´ Streamlit UI)
        self.enable_refine = True
        self.show_top5 = False
        self.mismatch_only = False

        self.conf = 0.15
        self.imgsz = 640

        self.short_map = {}      # CLASS_NAMES_SHORT
        self.refine_keys = []    # REFINE_WITH_CLS

        # fps smoothing
        self.fps_hist = deque(maxlen=30)

        # optional: expose latest rows to UI (n·∫øu b·∫°n mu·ªën show b·∫£ng)
        self.latest_rows = []

        

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img_bgr = frame.to_ndarray(format="bgr24")
        draw = img_bgr.copy()

        t0 = time.time()

        # --- YOLO detect (RGB) ---
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

        # quan tr·ªçng: kh√≥a model ƒë·ªÉ thread-safe
        with MODEL_LOCK:
            # d√πng yolo_model(...) ho·∫∑c yolo_model.predict(...) t√πy b·∫°n ƒëang vi·∫øt
            # c√°ch an to√†n nh·∫•t v·ªõi ultralytics:
            res = yolo_model.predict(img_rgb, conf=self.conf, imgsz=self.imgsz, verbose=False)[0]

        rows = []

        if res.boxes is not None and len(res.boxes) > 0:
            for box in res.boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                det_conf = float(box.conf[0])
                cls_id = int(box.cls[0])

                yolo_label = res.names[cls_id] if hasattr(res, "names") else str(cls_id)

                # --- classifier top-k (ƒë·ªÉ so YOLO vs CLS + top5 n·∫øu b·∫≠t) ---
                topk = 5 if self.show_top5 else 1
                preds = classify_crop_bgr(img_bgr, x1, y1, x2, y2, topk=topk)

                cls_top1_name, cls_top1_prob = (None, None)
                top5_str = ""

                if preds is not None and len(preds) > 0:
                    cls_top1_name, cls_top1_prob = preds[0]
                    if self.show_top5:
                        top5_str = ", ".join([f"{n} ({p:.2f})" for n, p in preds])

                # --- map short code tr∆∞·ªõc ƒë·ªÉ d√πng cho threshold theo class ---
                yolo_short = self.short_map.get(yolo_label, yolo_label)

                # classifier top1 short (ƒë·ªÉ so s√°nh / c·ª©u n·∫øu YOLO kh√¥ng ch·∫Øc)
                cls_short = ""
                if cls_top1_name:
                    cls_short = self.short_map.get(cls_top1_name, cls_top1_name)

                # --- DEMO-SAFE decision (ch·ªëng spam NO_STRAIGHT_RIGHT + tr√°nh nh√£n sai khi conf th·∫•p) ---
                min_conf_this = SPECIAL_MIN_CONF.get(yolo_short, DET_MIN_CONF)

                # b·ªè bbox qu√° y·∫øu (r√°c)
                too_weak = (det_conf < UNCERTAIN_MIN_CONF)


                allow_refine = any(k in yolo_label.lower() for k in self.refine_keys)
                use_refine = False

                # m·∫∑c ƒë·ªãnh: n·∫øu YOLO ch∆∞a ƒë·ªß tin -> hi·ªÉn th·ªã UNCERTAIN (v√†ng), tr√°nh nh√£n sai
                final_short = "UNCERTAIN"
                final_conf  = det_conf

                if too_weak:
                    final_short = "UNCERTAIN"
                    final_conf  = det_conf
                    use_refine  = False
                    mismatch    = False

                    row = {
                        "YOLO_label": yolo_short,
                        "CLS_top1": cls_short,
                        "Final_label": final_short,
                        "Det_conf": round(det_conf, 3),
                        "Final_conf": round(final_conf, 3),
                        "Refine_used": False,
                        "Mismatch": False,
                        "bbox": [x1, y1, x2, y2],
                    }
                    if self.show_top5:
                        row["Top5"] = top5_str

                    rows.append(row)
                    
                    continue


                # n·∫øu YOLO ƒë·ªß tin -> d√πng YOLO
                if det_conf >= min_conf_this:
                    final_short = yolo_short
                    final_conf  = det_conf

                # n·∫øu YOLO ch∆∞a ƒë·ªß tin nh∆∞ng classifier r·∫•t ch·∫Øc + cho ph√©p refine -> CLS c·ª©u
                if (final_short == "UNCERTAIN") and self.enable_refine and allow_refine and (cls_top1_name is not None):
                    if (cls_top1_prob is not None) and (float(cls_top1_prob) >= CLS_MIN_PROB):
                        final_short = cls_short
                        final_conf  = float(cls_top1_prob)
                        use_refine  = True

                # mismatch: ch·ªâ t√≠nh khi c√≥ cls_top1
                mismatch = (cls_top1_name is not None) and (yolo_short != cls_short) and (final_short != "UNCERTAIN")


                row = {
                    "YOLO_label": yolo_short,
                    "CLS_top1": cls_short,
                    "Final_label": final_short,
                    "Det_conf": round(det_conf, 3),
                    "Final_conf": round(final_conf, 3),
                    "Refine_used": bool(use_refine),
                    "Mismatch": bool(mismatch),
                    "bbox": [x1, y1, x2, y2],
                }

                if self.show_top5:
                    row["Top5"] = top5_str

                rows.append(row)

                # --- draw (bbox + label) ---
                if final_short == "UNCERTAIN":
                    color = (0, 255, 255)      # v√†ng
                elif mismatch:
                    color = (0, 0, 255)        # ƒë·ªè
                else:
                    color = (0, 255, 0)        # xanh

                cv2.rectangle(draw, (x1, y1), (x2, y2), color, 2)

                cv2.putText(
                    draw,
                    f"{final_short} ({final_conf:.2f})",
                    (x1, max(y1 - 6, 12)),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    color,
                    2,
                    cv2.LINE_AA
                )

        # --- filter mismatch only (UI option) ---
        if self.mismatch_only:
            rows_view = [r for r in rows if r.get("Mismatch")]
        else:
            rows_view = rows

        self.latest_rows = rows_view

        # --- FPS ---
        dt = max(time.time() - t0, 1e-6)
        fps = 1.0 / dt
        self.fps_hist.append(fps)
        fps_avg = sum(self.fps_hist) / len(self.fps_hist)
        self.fps_avg = fps_avg

        cv2.putText(
            draw,
            f"FPS: {fps_avg:.1f} | conf={self.conf:.2f} imgsz={self.imgsz}",
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.8,
            (255, 255, 255),
            2,
            cv2.LINE_AA
        )

        return av.VideoFrame.from_ndarray(draw, format="bgr24")

# ===========================================================
# ================ MODE 1: DETECTION (YOLO) =================
# ===========================================================
if mode.startswith("1Ô∏è‚É£"):
    st.subheader("üáªüá≥ Vietnam Traffic Sign Detection (YOLOv8)")
    st.markdown(
        "Ch·∫ø ƒë·ªô n√†y d√πng m√¥ h√¨nh **YOLOv8** ƒë√£ hu·∫•n luy·ªán tr√™n b·ªô d·ªØ li·ªáu bi·ªÉn b√°o Vi·ªát Nam "
        "ƒë·ªÉ **ph√°t hi·ªán v·ªã tr√≠** v√† **g√°n nh√£n** c√°c bi·ªÉn b√°o tr√™n ·∫£nh ho·∫∑c video."
    )

    st.markdown("#### 1. Ch·ªçn lo·∫°i d·ªØ li·ªáu ƒë·∫ßu v√†o")
    io_choice = st.radio(
        "Ki·ªÉu input:",
        ["Image", "Video"],
        horizontal=True
    )

    # D√πng 2 c·ªôt: tr√°i = upload, ph·∫£i = k·∫øt qu·∫£
    col_left, col_right = st.columns([1.1, 1.3])

    # ---------- IMAGE INPUT ----------
    if io_choice == "Image":
        with col_left:
            st.markdown("##### üì∑ Upload ·∫£nh")
            file = st.file_uploader(
                "Ch·ªçn m·ªôt ·∫£nh giao th√¥ng (JPG/PNG/JPEG):",
                type=["jpg", "png", "jpeg"],
                label_visibility="collapsed"
            )

        with col_right:
            st.markdown("##### üìä K·∫øt qu·∫£ nh·∫≠n di·ªán")

            if file is not None:
                temp_path = tempfile.NamedTemporaryFile(delete=False).name
                with open(temp_path, "wb") as f:
                    f.write(file.read())

                img_bgr = cv2.imread(temp_path)
                img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

                # --- ƒêO TH·ªúI GIAN TO√ÄN PIPELINE MODE 1 (·∫¢NH) ---
                start_time = time.time()

                # ch·∫°y detect YOLO
                results = yolo_model(img_rgb)[0]

                detections = []
                if len(results.boxes) == 0:
                    st.warning("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c bi·ªÉn b√°o n√†o trong ·∫£nh.")
                else:
                    for box in results.boxes:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        conf = float(box.conf[0])
                        cls_id = int(box.cls[0])

                        label = results.names[cls_id] if hasattr(results, "names") else str(cls_id)

                        detections.append({
                            "Label": label,
                            "Confidence": f"{conf:.2f}",
                            "Box [x1,y1,x2,y2]": f"[{x1}, {y1}, {x2}, {y2}]"
                        })

                        cv2.rectangle(img_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        cv2.putText(
                            img_bgr,
                            f"{label} ({conf:.2f})",
                            (x1, max(y1 - 5, 10)),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.5,
                            (0, 255, 0),
                            1,
                            cv2.LINE_AA
                        )

                    st.success(f"‚úÖ Ph√°t hi·ªán {len(detections)} bi·ªÉn b√°o:")
                    st.table(detections)

                # --- K·∫æT TH√öC ƒêO TH·ªúI GIAN ---
                end_time = time.time()
                elapsed_ms = (end_time - start_time) * 1000.0
                st.caption(f"[Mode 1] Processing time (YOLO only): {elapsed_ms:.1f} ms")

                st.image(
                    cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB),
                    caption="·∫¢nh sau khi nh·∫≠n di·ªán bi·ªÉn b√°o (YOLOv8)",
                    use_container_width=True
                )
            else:
                st.info("‚¨ÖÔ∏è H√£y upload m·ªôt ·∫£nh ·ªü c·ªôt b√™n tr√°i ƒë·ªÉ b·∫Øt ƒë·∫ßu.")


    # ---------- VIDEO INPUT ----------
    else:
        with col_left:
            st.markdown("##### üéûÔ∏è Upload video")
            file = st.file_uploader(
                "Ch·ªçn video (MP4/AVI/MOV), n√™n < 200MB:",
                type=["mp4", "avi", "mov"],
                label_visibility="collapsed"
            )
            st.caption("üí° Sau khi upload, video s·∫Ω ch·∫°y m·ªôt l·∫ßn v·ªõi khung live preview v√† b·∫£ng t·ªïng h·ª£p k·∫øt qu·∫£ ·ªü cu·ªëi.")

        with col_right:
            st.markdown("##### üìä Live preview & t·ªïng h·ª£p bi·ªÉn b√°o")

            if file is not None:
                st.info("‚è≥ ƒêang x·ª≠ l√Ω video, vui l√≤ng ch·ªù trong khi c√°c khung h√¨nh ƒë∆∞·ª£c x·ª≠ l√Ω...")

                tfile = tempfile.NamedTemporaryFile(delete=False)
                tfile.write(file.read())

                vidcap = cv2.VideoCapture(tfile.name)

                stframe = st.empty()
                status_box = st.empty()

                all_detections = {}
                frame_idx = 0

                # üîπ DANH S√ÅCH L∆ØU TH·ªúI GIAN/FPS M·ªñI FRAME
                frame_times = []

                while vidcap.isOpened():
                    ret, frame_bgr = vidcap.read()
                    if not ret:
                        break
                    frame_idx += 1

                    # --- B·∫ÆT ƒê·∫¶U ƒêO TH·ªúI GIAN CHO FRAME N√ÄY ---
                    t0 = time.time()

                    results = yolo_model(frame_bgr)[0]

                    draw_frame = frame_bgr.copy()
                    detections_this_frame = []

                    for box in results.boxes:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        conf = float(box.conf[0])
                        cls_id = int(box.cls[0])
                        label_name = results.names[cls_id]

                        cv2.rectangle(draw_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        cv2.putText(
                            draw_frame,
                            f"{label_name} ({conf:.2f})",
                            (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.5,
                            (0, 255, 0),
                            1,
                            cv2.LINE_AA
                        )

                        if label_name not in all_detections:
                            all_detections[label_name] = []
                        all_detections[label_name].append(conf)

                        detections_this_frame.append({
                            "class_name": label_name,
                            "confidence": round(conf, 4)
                        })

                    # --- K·∫æT TH√öC ƒêO TH·ªúI GIAN ---
                    t1 = time.time()
                    dt = t1 - t0           # gi√¢y / frame
                    if dt > 0:
                        fps = 1.0 / dt
                        frame_times.append(fps)

                    frame_rgb = cv2.cvtColor(draw_frame, cv2.COLOR_BGR2RGB)
                    stframe.image(
                        frame_rgb,
                        channels="RGB",
                        caption=f"Frame {frame_idx}",
                        use_container_width=True
                    )

                    if frame_idx % 5 == 0:
                        status_box.markdown(
                            f"üîÑ ƒêang x·ª≠ l√Ω frame **{frame_idx}** "
                            f"(ph√°t hi·ªán {len(detections_this_frame)} bi·ªÉn b√°o trong khung h√¨nh g·∫ßn nh·∫•t)"
                        )

                vidcap.release()

                # üîπ SAU KHI X·ª¨ L√ù XONG VIDEO ‚Üí T√çNH FPS TRUNG B√åNH
                avg_fps = None
                if len(frame_times) > 0:
                    avg_fps = sum(frame_times) / len(frame_times)
                    st.caption(f"[Mode 1] Average FPS (YOLO only, video): {avg_fps:.2f}")

                st.success("‚úÖ Video ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω xong. T·ªïng h·ª£p c√°c bi·ªÉn b√°o ph√°t hi·ªán ƒë∆∞·ª£c:")

                if len(all_detections) == 0:
                    st.write("_Kh√¥ng c√≥ bi·ªÉn b√°o n√†o ƒë∆∞·ª£c ph√°t hi·ªán trong to√†n b·ªô video._")
                else:
                    summary_rows = []
                    for label_name, conf_list in all_detections.items():
                        max_conf = max(conf_list)
                        count = len(conf_list)
                        summary_rows.append((label_name, max_conf, count))

                    summary_rows.sort(key=lambda x: x[1], reverse=True)

                    table_lines = [
                        "| Bi·ªÉn b√°o | ƒê·ªô tin c·∫≠y cao nh·∫•t | S·ªë l·∫ßn xu·∫•t hi·ªán |",
                        "|----------|---------------------|------------------|",
                    ]
                    for (label_name, max_conf, count) in summary_rows:
                        table_lines.append(
                            f"| {label_name} | {max_conf:.2f} | {count} |"
                        )

                    st.markdown("\n".join(table_lines))
            else:
                st.info("‚¨ÖÔ∏è H√£y upload m·ªôt video ·ªü c·ªôt b√™n tr√°i ƒë·ªÉ b·∫Øt ƒë·∫ßu.")


# ===========================================================
# =========== MODE 2: CLASSIFICATION (GTSRB) ================
# ===========================================================
elif mode.startswith("2Ô∏è‚É£"):
    st.subheader("üö¶ GTSRB Traffic Sign Classification (EfficientNet-B0)")
    st.markdown(
        "Ch·∫ø ƒë·ªô n√†y d√πng m√¥ h√¨nh **EfficientNet-B0 fine-tune tr√™n GTSRB** ƒë·ªÉ ph√¢n lo·∫°i "
        "·∫£nh **bi·ªÉn b√°o ƒë√£ ƒë∆∞·ª£c crop s·∫µn** v√†o 1 trong 43 l·ªõp."
    )

    col_left, col_right = st.columns([1.1, 1.3])

    with col_left:
        st.markdown("##### üì∑ Upload ·∫£nh bi·ªÉn b√°o (crop)")
        img_file = st.file_uploader(
            "Ch·ªçn ·∫£nh (JPG/PNG/JPEG):",
            type=["jpg", "png", "jpeg"],
            label_visibility="collapsed"
        )
        st.caption("üí° N√™n d√πng ·∫£nh ch·ªâ ch·ª©a ri√™ng bi·ªÉn b√°o, kh√¥ng ch·ª©a nhi·ªÅu background.")

    with col_right:
        st.markdown("##### üìä K·∫øt qu·∫£ ph√¢n lo·∫°i")

        if img_file is not None:
            pil_img = Image.open(img_file).convert("RGB")

            st.image(
                pil_img,
                caption="·∫¢nh input (bi·ªÉn b√°o ƒë√£ crop)",
                use_container_width=True
            )

            # --- B·∫ÆT ƒê·∫¶U ƒêO TH·ªúI GIAN MODE 2 (CLASSIFIER ONLY) ---
            start_time = time.time()

            x = clf_transform(pil_img).unsqueeze(0).to(DEVICE)

            with torch.no_grad():
                logits = clf_model(x)
                probs = F.softmax(logits, dim=1)
                top_prob, top_idx = torch.max(probs, dim=1)
                top_prob = float(top_prob.item())
                top_idx  = int(top_idx.item())

            # --- K·∫æT TH√öC ƒêO TH·ªúI GIAN ---
            end_time = time.time()
            elapsed_ms = (end_time - start_time) * 1000.0

            class_name = CLASS_NAMES[top_idx] if top_idx < len(CLASS_NAMES) else f"class_{top_idx}"

            st.success("‚úÖ Classification Result")
            st.markdown(f"- **Predicted class ID**: `{top_idx}`")
            st.markdown(f"- **Class name**: **{class_name}**")
            st.markdown(f"- **Confidence**: `{top_prob:.4f}`")

            # üëâ Th√™m d√≤ng n√†y ƒë·ªÉ hi·ªÉn th·ªã th·ªùi gian th·ª±c thi Mode 2
            st.caption(f"[Mode 2] Processing time (Classifier only): {elapsed_ms:.1f} ms")

            topk = 5
            top_probs, top_indices = torch.topk(probs[0], k=min(topk, probs.shape[1]))
            top_view = []
            for rank in range(len(top_probs)):
                idx_i = int(top_indices[rank].item())
                prob_i = float(top_probs[rank].item())
                name_i = CLASS_NAMES[idx_i] if idx_i < len(CLASS_NAMES) else f"class_{idx_i}"
                top_view.append({
                    "Rank": rank + 1,
                    "ClassID": idx_i,
                    "Name": name_i,
                    "Confidence": f"{prob_i:.4f}"
                })

            st.markdown("##### üîù Top-k d·ª± ƒëo√°n")
            st.table(top_view)
        else:
            st.info("‚¨ÖÔ∏è H√£y upload m·ªôt ·∫£nh bi·ªÉn b√°o ·ªü c·ªôt b√™n tr√°i ƒë·ªÉ xem k·∫øt qu·∫£ ph√¢n lo·∫°i.")

# ===========================================================
# ============ MODE 3: UNIFIED PIPELINE (YOLO + CLS) ==========
else:
    st.header("üîó Unified Pipeline: YOLOv8 + EfficientNet-B0")
    
    st.subheader("‚öôÔ∏è Options")
    colA, colB, colC = st.columns([1,1,1])
    with colA:
        ENABLE_REFINE = st.checkbox("Enable refine (YOLO ‚Üí CLS)", value=True)
    with colB:
        SHOW_TOP5 = st.checkbox("Show Top-5 in table", value=True)
    with colC:
        SHOW_MISMATCH_ONLY = st.checkbox("Show mismatch only", value=False)

    st.caption("Ghi ch√∫: Refine = d√πng Classifier Top-1 ƒë·ªÉ thay nh√£n YOLO (theo rule REFINE_WITH_CLS).")

    io_choice = st.radio("Select input type", ["Image", "Video", "Webcam"])

    # ========================= IMAGE MODE =========================
    if io_choice == "Image":
        uploaded_file = st.file_uploader("Upload an image", type=["jpg", "png", "jpeg"])

        if uploaded_file is not None:
            file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
            img_bgr = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

            st.image(img_bgr, channels="BGR", caption="Input Image")

            # --- ƒêO TH·ªúI GIAN TO√ÄN PIPELINE MODE 3 (·∫¢NH) ---
            start_time = time.time()

            # --- YOLO DETECT ---
            with st.spinner("Running YOLOv8 detection..."):
                results = yolo_model(img_rgb)[0]

            unified_detections = []
            out_img = img_bgr.copy()

            if len(results.boxes) == 0:
                st.warning("No traffic sign detected.")
            else:
                for box in results.boxes:
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    det_conf = float(box.conf[0])
                    cls_id   = int(box.cls[0])

                    yolo_label = results.names[cls_id] if hasattr(results, "names") else str(cls_id)

                    # lu√¥n ch·∫°y classifier ƒë·ªÉ l·∫•y Top-5 (ph·ª•c v·ª• hi·ªÉn th·ªã)
                    preds = classify_crop_bgr(out_img, x1, y1, x2, y2, topk=5)

                    # build chu·ªói Top-5
                    top5_str = ""
                    cls_top1_name, cls_top1_prob = None, None
                    if preds is not None and len(preds) > 0:
                        cls_top1_name, cls_top1_prob = preds[0]
                        if SHOW_TOP5:
                            top5_str = ", ".join([f"{name} ({prob:.2f})" for name, prob in preds])

                    # quy·∫øt ƒë·ªãnh refine hay kh√¥ng
                    allow_refine_by_rule = any(key in yolo_label.lower() for key in REFINE_WITH_CLS)
                    use_refine = ENABLE_REFINE and (cls_top1_name is not None) and allow_refine_by_rule

                    final_label = cls_top1_name if use_refine else yolo_label
                    final_conf  = float(cls_top1_prob) if use_refine else det_conf

                    # mapping sang short label (classes_short.json)
                    yolo_short  = CLASS_NAMES_SHORT.get(yolo_label, yolo_label)
                    cls_short   = CLASS_NAMES_SHORT.get(cls_top1_name, cls_top1_name) if cls_top1_name else ""
                    final_short = CLASS_NAMES_SHORT.get(final_label, final_label)

                    mismatch = (cls_top1_name is not None) and (CLASS_NAMES_SHORT.get(yolo_label, yolo_label) != CLASS_NAMES_SHORT.get(cls_top1_name, cls_top1_name))

                    row = {
                        "YOLO_label": yolo_short,
                        "CLS_top1": cls_short,
                        "Final_label": final_short,
                        "Det_conf": round(det_conf, 3),
                        "Final_conf": round(final_conf, 3),
                        "Refine_used": bool(use_refine),
                        "Mismatch": bool(mismatch),
                        "bbox": [x1, y1, x2, y2],
                    }
                    if SHOW_TOP5:
                        row["Top5"] = top5_str

                    unified_detections.append(row)

                    # DRAW
                    cv2.rectangle(out_img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(
                        out_img,
                        f"{final_short} ({final_conf:.2f})",
                        (x1, max(y1 - 5, 10)),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.5,
                        (0, 255, 0),
                        1,
                        cv2.LINE_AA
                    )


                st.subheader("Unified Output")
                st.image(out_img, channels="BGR")

                st.subheader("YOLO vs Classifier Results")
                df = pd.DataFrame(unified_detections)

                if SHOW_MISMATCH_ONLY and "Mismatch" in df.columns:
                    df_view = df[df["Mismatch"] == True].copy()
                else:
                    df_view = df

                st.dataframe(df_view, use_container_width=True)

                # Export CSV
                csv_bytes = df_view.to_csv(index=False).encode("utf-8")
                st.download_button(
                    "‚¨áÔ∏è Download results CSV",
                    data=csv_bytes,
                    file_name="mode3_unified_results.csv",
                    mime="text/csv"
                )

                # --- K·∫æT TH√öC ƒêO TH·ªúI GIAN ---
                end_time = time.time()
                elapsed_ms = (end_time - start_time) * 1000.0
                st.caption(f"[Mode 3] Processing time (YOLO + CLS + rule): {elapsed_ms:.1f} ms")

    # ========================= VIDEO MODE =========================
    elif io_choice == "Video":
        uploaded_video = st.file_uploader("Upload a video", type=["mp4","avi","mov"])

        if uploaded_video is not None:
            tfile = tempfile.NamedTemporaryFile(delete=False)
            tfile.write(uploaded_video.read())

            cap = cv2.VideoCapture(tfile.name)
            stframe = st.empty()

            all_stats = {}            # summary theo Final_label
            fps_list = []             # FPS t·ª´ng frame
            frame_time_list = []      # th·ªùi gian x·ª≠ l√Ω t·ª´ng frame (seconds)
            video_rows = []           # log csv cho t·ª´ng detection
            frame_idx = 0

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                frame_idx += 1

                t0 = time.time()

                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = yolo_model(frame_rgb)[0]
                draw = frame.copy()

                # N·∫øu kh√¥ng c√≥ box ‚Üí v·∫´n t√≠nh time + fps + show frame
                if len(results.boxes) == 0:
                    dt = time.time() - t0
                    frame_time_list.append(dt)
                    fps = (1.0 / dt) if dt > 0 else 0.0
                    fps_list.append(fps)

                    cv2.putText(draw, f"FPS: {fps:.1f}", (10, 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)
                    stframe.image(draw, channels="BGR")
                    continue

                for box in results.boxes:
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    det_conf = float(box.conf[0])
                    cls_id   = int(box.cls[0])

                    yolo_label = results.names[cls_id] if hasattr(results, "names") else str(cls_id)

                    # 1) lu√¥n ch·∫°y classifier ƒë·ªÉ l·∫•y Top-5 (ƒë·ªÉ so s√°nh YOLO vs CLS + mismatch)
                    preds = classify_crop_bgr(draw, x1, y1, x2, y2, topk=5)

                    top5_str = ""
                    cls_top1_name, cls_top1_prob = None, None
                    if preds is not None and len(preds) > 0:
                        cls_top1_name, cls_top1_prob = preds[0]
                        if SHOW_TOP5:
                            top5_str = ", ".join([f"{name} ({prob:.2f})" for name, prob in preds])

                    # 2) b·∫≠t/t·∫Øt refine + rule refine
                    allow_refine_by_rule = any(key in yolo_label.lower() for key in REFINE_WITH_CLS)
                    use_refine = ENABLE_REFINE and (cls_top1_name is not None) and allow_refine_by_rule

                    final_label = cls_top1_name if use_refine else yolo_label
                    final_conf  = float(cls_top1_prob) if use_refine else det_conf

                    # 3) map sang short label
                    yolo_short  = CLASS_NAMES_SHORT.get(yolo_label, yolo_label)
                    cls_short   = CLASS_NAMES_SHORT.get(cls_top1_name, cls_top1_name) if cls_top1_name else ""
                    final_short = CLASS_NAMES_SHORT.get(final_label, final_label)

                    mismatch = (cls_top1_name is not None) and (yolo_short != cls_short)

                    # 4) highlight mismatch (bbox ƒë·ªè n·∫øu mismatch, xanh n·∫øu kh·ªõp)
                    color = (0, 0, 255) if mismatch else (0, 255, 0)

                    # DRAW
                    cv2.rectangle(draw, (x1, y1), (x2, y2), color, 2)
                    cv2.putText(
                        draw,
                        f"{final_short} ({final_conf:.2f})",
                        (x1, max(y1 - 5, 10)),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.5,
                        color,
                        1,
                        cv2.LINE_AA
                    )

                    # summary stats
                    if final_short not in all_stats:
                        all_stats[final_short] = []
                    all_stats[final_short].append(float(final_conf))

                    # log row (CSV)
                    row = {
                        "frame": frame_idx,
                        "YOLO_label": yolo_short,
                        "CLS_top1": cls_short,
                        "Final_label": final_short,
                        "Det_conf": round(det_conf, 3),
                        "Final_conf": round(float(final_conf), 3),
                        "Refine_used": bool(use_refine),
                        "Mismatch": bool(mismatch),
                        "x1": x1, "y1": y1, "x2": x2, "y2": y2,
                    }
                    if SHOW_TOP5:
                        row["Top5"] = top5_str
                    video_rows.append(row)

                # FPS + frame time (t√≠nh 1 l·∫ßn sau khi x·ª≠ l√Ω xong frame)
                dt = time.time() - t0
                frame_time_list.append(dt)

                fps = (1.0 / dt) if dt > 0 else 0.0
                fps_list.append(fps)

                # V·∫Ω FPS l√™n khung h√¨nh
                cv2.putText(draw, f"FPS: {fps:.1f}", (10, 25),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)

                stframe.image(draw, channels="BGR")

            cap.release()

            # ====== AVG FPS + AVG FRAME TIME ======
            if len(fps_list) > 0:
                avg_fps = sum(fps_list) / len(fps_list)
                st.caption(f"[Mode 3] Average FPS (YOLO + CLS + rule, video): {avg_fps:.2f}")

            if len(frame_time_list) > 0:
                avg_frame_time_ms = (sum(frame_time_list) / len(frame_time_list)) * 1000.0
                st.caption(f"[Mode 3] Average frame time: {avg_frame_time_ms:.1f} ms/frame")

            # ====== SUMMARY TABLE ======
            if len(all_stats) > 0:
                st.subheader("Summary (Unified Pipeline)")
                table = [{"Label": k, "Avg confidence": sum(v)/len(v), "Count": len(v)} for k, v in all_stats.items()]
                st.table(table)

            # ====== CSV DOWNLOAD (l·ªçc mismatch n·∫øu ch·ªçn) ======
            if len(video_rows) > 0:
                dfv = pd.DataFrame(video_rows)
                if SHOW_MISMATCH_ONLY and "Mismatch" in dfv.columns:
                    dfv = dfv[dfv["Mismatch"] == True].copy()

                st.download_button(
                    "‚¨áÔ∏è Download video log CSV",
                    data=dfv.to_csv(index=False).encode("utf-8"),
                    file_name="mode3_video_log.csv",
                    mime="text/csv"
                )

    else:
        st.subheader("üì∑ Webcam realtime (Mode 3)")

        # --- UI options gi·ªëng Mode 3 --- (NH·ªö key ri√™ng)
        col1, col2, col3 = st.columns(3)
        with col1:
            enable_refine_rt = st.checkbox(
                "Enable refine (YOLO ‚Üí CLS)",
                value=True,
                key="m3_webcam_enable_refine",
            )
        with col2:
            show_top5_rt = st.checkbox(
                "Show Top-5 (slower)",
                value=False,
                key="m3_webcam_show_top5",
            )
        with col3:
            mismatch_only_rt = st.checkbox(
                "Show mismatch only",
                value=False,
                key="m3_webcam_mismatch_only",
            )

        st.caption("G·ª£i √Ω: Webcam th∆∞·ªùng blur/nh·ªè ‚Üí th·ª≠ conf=0.10‚Äì0.20 v√† imgsz=640‚Äì960.")

        conf_rt = st.slider(
            "YOLO conf",
            0.05, 0.80, 0.15, 0.05,
            key="m3_webcam_conf",
        )
        imgsz_rt = st.selectbox(
            "YOLO imgsz",
            [320, 480, 640, 960, 1280],
            index=2,
            key="m3_webcam_imgsz",
        )

        webrtc_ctx = webrtc_streamer(
            key="m3_webcam_stream",  # key ri√™ng lu√¥n
            video_processor_factory=Mode3WebcamProcessor,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

        # placeholders ƒë·ªÉ update UI
        ph_fps = st.empty()
        ph_tbl = st.empty()

        AUTO_UPDATE = st.checkbox("Auto update results (webcam)", value=True, key="m3_webcam_autoupdate")

        if webrtc_ctx.state.playing and webrtc_ctx.video_processor:
            # (tu·ª≥ b·∫°n) hi·ªán FPS/Result realtime khi webcam ƒëang ch·∫°y
            while AUTO_UPDATE and webrtc_ctx.state.playing:
                vp = webrtc_ctx.video_processor

                rows = getattr(vp, "latest_rows", []) or []
                df = pd.DataFrame(rows)

                # n·∫øu b·∫°n c√≥ l∆∞u fps_avg trong processor th√¨ show, c√≤n kh√¥ng th√¨ b·ªè
                fps_avg = getattr(vp, "fps_avg", None)
                if fps_avg is not None:
                    ph_fps.caption(f"[Mode 3 - Webcam] FPS(avg): {fps_avg:.1f}")
                else:
                    ph_fps.caption("[Mode 3 - Webcam] Running...")

                if df.empty:
                    ph_tbl.info("Ch∆∞a c√≥ detection n√†o (ho·∫∑c ƒëang b·ªã filter).")
                else:
                    ph_tbl.dataframe(df, use_container_width=True)

                time.sleep(0.2)  # 5 l·∫ßn/gi√¢y l√† ƒë·ªß m∆∞·ª£t


        # g√°n option v√†o processor (sau khi stream kh·ªüi t·∫°o)
        if webrtc_ctx.video_processor:
            vp = webrtc_ctx.video_processor
            vp.enable_refine = enable_refine_rt
            vp.show_top5 = show_top5_rt
            vp.mismatch_only = mismatch_only_rt
            vp.conf = conf_rt
            vp.imgsz = imgsz_rt

            # mapping + rule (b·∫Øt bu·ªôc ph·∫£i c√≥ s·∫µn ngo√†i scope)
            vp.short_map = CLASS_NAMES_SHORT
            vp.refine_keys = REFINE_WITH_CLS

            st.subheader("Webcam results (latest frame)")

            if st.button("üîÑ Refresh results", key="btn_refresh_webcam"):
                pass  # b·∫•m n√∫t ƒë·ªÉ Streamlit rerun v√† k√©o d·ªØ li·ªáu m·ªõi

            rows = []
            if webrtc_ctx.video_processor:
                # copy ra ƒë·ªÉ tr√°nh thread ƒëang update
                rows = list(getattr(webrtc_ctx.video_processor, "latest_rows", []))

            df = pd.DataFrame(rows)
            st.dataframe(df, use_container_width=True)

            if not df.empty:
                st.download_button(
                    "‚¨áÔ∏è Download CSV (latest frame)",
                    data=df.to_csv(index=False).encode("utf-8"),
                    file_name="mode3_webcam_latest.csv",
                    mime="text/csv"
                )




